{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed54fbb",
   "metadata": {},
   "source": [
    "# Product Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece93d84",
   "metadata": {},
   "source": [
    "Here we will use two techniques, which are, KMeans clustering and Deep learning based classification so as to segment and classify different types of bottles coming out of the same manufacturing belt/line and allow them to be seperated by the mechanism present in the line, for the purpose of making batches or processing them differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e632121",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb0882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import glob\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90ba8d",
   "metadata": {},
   "source": [
    "### Defining Some File info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83500d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = 'D:/My Work/Mechatronics/8th Sem. PPTs and Notes/Models/Product Segmentation/KNN Images/'\n",
    "out_file = 'D:/My Work/Mechatronics/8th Sem. PPTs and Notes/Models/Product Segmentation/output/'\n",
    "buf_file = 'D:/My Work/Mechatronics/8th Sem. PPTs and Notes/Models/Product Segmentation/buffer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9f1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting old test images\n",
    "def get_ready():\n",
    "    for img in os.listdir(img_file):\n",
    "        path = os.path.join(img_file, img)\n",
    "        os.remove(path)\n",
    "    shutil.rmtree(out_file)\n",
    "    os.mkdir('output')\n",
    "    shutil.rmtree(buf_file)\n",
    "    os.mkdir('buffer')\n",
    "    return img_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21071747",
   "metadata": {},
   "source": [
    "### Segment Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c51ab5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(img,k):\n",
    "    # Change color to RGB (from BGR)\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Reshaping the image into a 2D array of pixels and 3 color values (RGB)\n",
    "    pixel_vals = image.reshape((-1,3))\n",
    "    # Convert to float type\n",
    "    pixel_vals = np.float32(pixel_vals)\n",
    "    #the below line of code defines the criteria for the algorithm to stop running,\n",
    "    #which will happen is 100 iterations are run or the epsilon (which is the required accuracy)\n",
    "    #becomes 85%\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)\n",
    " \n",
    "    # then perform k-means clustering with number of clusters defined as 3\n",
    "    #also random centres are initially choosed for k-means clustering\n",
    "    \n",
    "    retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    " \n",
    "    # convert data into 8-bit values\n",
    "    centers = np.uint8(centers)\n",
    "    segmented_data = centers[labels.flatten()]\n",
    " \n",
    "    # reshape data into the original image dimensions\n",
    "    segmented_image = segmented_data.reshape((image.shape))\n",
    "    \n",
    "    print('Image Segmented')\n",
    "    \n",
    "    cv2.imwrite(buf_file + 'img.jpg', segmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f3047",
   "metadata": {},
   "source": [
    "### Extracting each bottle from test image via yoloV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee14270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(img,K):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # reading the pretrained yoloV3 files\n",
    "    img = cv2.resize(img, None, fx=1, fy=1)\n",
    "    net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "    # reading and storing the classes on which yolo is trained on\n",
    "    classes = []\n",
    "    with open(\"coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    #get layers of the network\n",
    "    layer_names = net.getLayerNames()\n",
    "    #Determine the output layer names from the YOLO model \n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    print(\"YOLO LOADED\")\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    # USing blob function of opencv to preprocess image\n",
    "    blob = cv2.dnn.blobFromImage(img, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "    #Detecting objects\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Finding and detecting objects and assigning the bounding boxes\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.65:\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    #We use NMS function in opencv to perform Non-maximum Suppression\n",
    "    #we give it score threshold and nms threshold as arguments.\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "    \n",
    "    # Now storing the images separately\n",
    "    count=0\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            color = colors[class_ids[i]]\n",
    "            img_store = img[y:y+h,x:x+w]\n",
    "            if img_store.any() and label=='bottle':\n",
    "                cv2.imwrite(img_file + 'img-' + str(count) + '.jpg', img_store)\n",
    "                count+=1\n",
    "                \n",
    "    # we will feed all the images to the KMeans classifier\n",
    "    preds = Kmeans(K)\n",
    "    count=0\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            color = (255,255,255)\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 1)\n",
    "            if img[y:y+h,x:x+w].any() and label=='bottle':\n",
    "                cv2.putText(img, label + 'Cluster:' + str(preds[count]), (x, y+20),cv2.FONT_HERSHEY_SIMPLEX,0.75, color, 2)\n",
    "                count+=1\n",
    "    cv2.imshow(\"Image\",img)\n",
    "    cv2.imwrite(\"D:/My Work/Mechatronics/8th Sem. PPTs and Notes/Models/Product Segmentation/final/img.jpg\", img)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccefc1",
   "metadata": {},
   "source": [
    "### Doing feature extraction from each individual image and KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20c8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans(k=5):\n",
    "    glob_dir = img_file + '/*.jpg'\n",
    "    images = [cv2.resize(cv2.imread(file), (224, 224)) for file in glob.glob(glob_dir)]\n",
    "    paths = [file for file in glob.glob(glob_dir)]\n",
    "    images = np.array(np.float32(images).reshape(len(images), -1)/255)\n",
    "    model = tf.keras.applications.MobileNetV2(include_top=False,\n",
    "    weights='imagenet', input_shape=(224, 224, 3))\n",
    "    predictions = model.predict(images.reshape(-1, 224, 224, 3))\n",
    "    print('features extracted via mobilenet')\n",
    "    pred_images = predictions.reshape(images.shape[0], -1)\n",
    "    kmodel = KMeans(n_clusters = k,init = 'random',n_init = 'auto',max_iter=1000, tol = 0.001, random_state=42)\n",
    "    kmodel.fit(pred_images)\n",
    "    kpredictions = kmodel.predict(pred_images)\n",
    "    shutil.rmtree('output')\n",
    "    print('kmeans predictions done')\n",
    "    for i in range(k):\n",
    "        os.makedirs('output\\cluster' + str(i))\n",
    "    for i in range(len(paths)):\n",
    "        shutil.copy2(paths[i], 'output\\cluster'+str(kpredictions[i]))\n",
    "    return kpredictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943ced2",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a796ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realtime_img(img,K):\n",
    "    get_ready()\n",
    "    #segment(img,3)\n",
    "    #img = cv2.imread('buffer/img.jpg')\n",
    "    get_clusters(img,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2300506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO LOADED\n",
      "1/1 [==============================] - 1s 830ms/step\n",
      "features extracted via mobilenet\n",
      "kmeans predictions done\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "img = cv2.imread(\"Test Images/img-4.jpg\")\n",
    "realtime_img(img,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac9f5b7",
   "metadata": {},
   "source": [
    "### Real-time Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5d7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realtime_vid(K):\n",
    "    get_ready()\n",
    "    net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "    if True:\n",
    "        # set CUDA as the preferable backend and target\n",
    "        print(\"[INFO] setting preferable backend and target to CUDA...\")\n",
    "        net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "        net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "    #save all the names in file o the list classes\n",
    "    classes = []\n",
    "    kpredictions=[]\n",
    "    with open(\"coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    #get layers of the network\n",
    "    layer_names = net.getLayerNames()\n",
    "    #Determine the output layer names from the YOLO model \n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        get_ready()\n",
    "        # Capture frame-by-frame\n",
    "        re,img = video_capture.read()\n",
    "        if img.any():\n",
    "            img = cv2.resize(img, None, fx=1.64, fy=1)\n",
    "            height, width, channels = img.shape\n",
    "\n",
    "            # USing blob function of opencv to preprocess image\n",
    "            blob = cv2.dnn.blobFromImage(img, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "            #Detecting objects\n",
    "            net.setInput(blob)\n",
    "            outs = net.forward(output_layers)\n",
    "\n",
    "            # Showing informations on the screen\n",
    "            class_ids = []\n",
    "            confidences = []\n",
    "            boxes = []\n",
    "            for out in outs:\n",
    "                for detection in out:\n",
    "                    scores = detection[5:]\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "                    if confidence > 0.6:\n",
    "                        # Object detected\n",
    "                        center_x = int(detection[0] * width)\n",
    "                        center_y = int(detection[1] * height)\n",
    "                        w = int(detection[2] * width)\n",
    "                        h = int(detection[3] * height)\n",
    "\n",
    "                        # Rectangle coordinates\n",
    "                        x = int(center_x - w / 2)\n",
    "                        y = int(center_y - h / 2)\n",
    "\n",
    "                        boxes.append([x, y, w, h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        class_ids.append(class_id)\n",
    "\n",
    "            #We use NMS function in opencv to perform Non-maximum Suppression\n",
    "            #we give it score threshold and nms threshold as arguments.\n",
    "            indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "            font = cv2.FONT_HERSHEY_PLAIN\n",
    "            colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "            # Now storing the images separately\n",
    "            count=0\n",
    "            for i in range(len(boxes)):\n",
    "                if i in indexes:\n",
    "                    x, y, w, h = boxes[i]\n",
    "                    label = str(classes[class_ids[i]])\n",
    "                    color = colors[class_ids[i]]\n",
    "                    img_store = img[y:y+h,x:x+w]\n",
    "                    if img_store.any() and label=='bottle':\n",
    "                        cv2.imwrite(img_file + 'img' + str(count) + '.jpg', img_store)\n",
    "                        count+=1\n",
    "\n",
    "            # Kmeans\n",
    "            glob_dir = img_file + '/*.jpg'\n",
    "            images = [cv2.resize(cv2.imread(file), (224, 224)) for file in glob.glob(glob_dir)]\n",
    "            paths = [file for file in glob.glob(glob_dir)]\n",
    "            if len(images)!=0 :   \n",
    "                pred_images = np.array(np.float32(images).reshape(len(images), -1)/255)\n",
    "                #model = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "                #predictions = model.predict(images.reshape(-1, 224, 224, 3))\n",
    "                #pred_images = images.reshape(-1, 224, 224, 3)\n",
    "                #pred_images = predictions.reshape(images.shape[0], -1)\n",
    "                kmodel = KMeans(n_clusters = K,init = 'random',n_init = 'auto',max_iter=1000, tol = 0.001, random_state=42)\n",
    "                print(count)\n",
    "                if kmodel.n_clusters<=count:\n",
    "                    kmodel.fit(pred_images)\n",
    "                    kpredictions = kmodel.predict(pred_images)\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            for i in range(len(boxes)):\n",
    "                if i in indexes:\n",
    "                    x, y, w, h = boxes[i]\n",
    "                    label = str(classes[class_ids[i]])\n",
    "                    color = colors[class_ids[i]]\n",
    "                    cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "                    if img[y:y+h,x:x+w].any()  and label=='bottle' and len(kpredictions)>0:\n",
    "                        cv2.putText(img, label + 'Cluster:' + str(kpredictions[count]), (x, y+20),cv2.FONT_HERSHEY_SIMPLEX,0.75, color, 2)\n",
    "                        count+=1\n",
    "            cv2.imshow(\"Image\",cv2.resize(img, (800,600)))\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bbd50a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] setting preferable backend and target to CUDA...\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "realtime_vid(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e4af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
